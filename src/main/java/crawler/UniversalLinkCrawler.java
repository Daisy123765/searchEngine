package crawler;import java.io.IOException;import java.util.ArrayList;import java.util.List;import java.util.regex.Matcher;import java.util.regex.Pattern;import org.apache.http.HttpEntity;import org.apache.http.client.methods.CloseableHttpResponse;import org.apache.http.client.methods.HttpGet;import org.apache.http.client.protocol.HttpClientContext;import org.apache.http.impl.client.CloseableHttpClient;import org.apache.http.impl.client.HttpClients;import org.apache.http.impl.conn.PoolingHttpClientConnectionManager;import org.apache.http.protocol.HttpContext;import org.apache.http.util.EntityUtils;import org.jsoup.Jsoup;import org.jsoup.nodes.Document;import org.jsoup.nodes.Element;import org.jsoup.select.Elements;public class UniversalLinkCrawler extends Thread{	private final CloseableHttpClient httpClinet;	private final HttpContext context;	private final HttpGet httpGet;	String cssSelector;	String url;	List<String> resultLists = new ArrayList<>();		public UniversalLinkCrawler(CloseableHttpClient httpClient, String url, String cssSelector) {		this.httpClinet = httpClient;		this.context = HttpClientContext.create();		if(url.charAt(url.length() - 1) != '/') {			url += '/';		}		this.url = url;		this.httpGet = new HttpGet(url);		this.cssSelector = cssSelector;	}		public void setUrl(String url) {		this.url = url;	}		public void setCssSelector(String cssSelector) {		this.cssSelector = cssSelector;	}		public List<String> getResultLists() {		return resultLists;	}		@Override	public void run() {		try {			CloseableHttpResponse response = httpClinet.execute(httpGet, context);			try {				HttpEntity entity = response.getEntity();				Document doc = Jsoup.parse(EntityUtils.toString(entity, "utf8"));				Elements links = doc.select(cssSelector);				for(Element link : links) {					String newUrl = link.attr("href");					if(newUrl.length() == 0) {						continue;					}					String pattern = ":.+";					String[] protocols = {"http(s)?","file","javascript"};					/**					 * 首先判断是不是http,https,file或者其他协议，防止css选择器找不到我们要的元素					 */					boolean flag = true;					for(String protocol : protocols) {						Pattern protocolPattern = Pattern.compile("^"+protocol + pattern);						Matcher protocolMatcher = protocolPattern.matcher(newUrl);						if(protocolMatcher.find()) {							flag = false;							break;						}					}					if(flag == false) continue;					/**					 * "/path"形式的连接，代表跳转到网站根目录下					 */					if(newUrl.length() != 0 && newUrl.charAt(0) == '/' && (newUrl.length() == 1 || newUrl.charAt(1) !=	 '/')) {						Pattern p = Pattern.compile("http(s)?://.+?/");						Matcher m = p.matcher(this.url);						String rootUrl = null;						if(m.find()) {							rootUrl = m.group();						}						else {							continue;						}						newUrl = rootUrl + newUrl.substring(1);					}					/**					 * "path"形式或者"./path"的href，意为当前目录下的资源					 */					else if(newUrl.length() != 0 && (newUrl.charAt(0) != '/' || (newUrl.charAt(0) == '.' && newUrl.charAt(1) == '/'))) {						newUrl = this.url + newUrl;					}					this.resultLists.add(newUrl);				}			} finally {				response.close();			}		} catch (IOException ex) {			ex.printStackTrace();		}		System.out.println(this.getResultLists().size());		for(String e : this.resultLists) {			System.out.println(e);		}	}		public static void main(String[] args) {		PoolingHttpClientConnectionManager cm = new PoolingHttpClientConnectionManager();		CloseableHttpClient httpClient = HttpClients.custom()		        .setConnectionManager(cm)		        .build();		UniversalLinkCrawler task = new UniversalLinkCrawler(httpClient, "http://cec.jmu.edu.cn/", "a[href]");		task.start();		//UniversalLinkCrawler a = new UniversalLinkCrawler("http://cec.jmu.edu.cn", ".menu0_0_");	}}